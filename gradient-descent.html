<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />

  <meta name="google-site-verification" content="yQh84bCkno9EjLcuqA5P2dztLWkwgr5WVqTJa9Za_6g">

  <title>What is Gradient Descent?</title>


  <meta property="og:title" content="What is Gradient Descent?" />
  <meta property="og:type" content="article" />
  <meta property="og:image" content="/images/descending.jpg" />
  <meta property="og:url" content="https://chrisdinant.github.io/gradient-descent.html" />
  <meta property="og:description" content="Fitting a machine learning model means finding optimal values for the parameters of the model. Sometimes this can be done in one step (when a closed form solution is available), but more often than not even if this can be done it is computationally prohibitively expensive. That is why most â€¦" />
  <meta property="og:locale" content="" />
  <meta property="og:site_name" content="Transferred Learnings" />
  <meta property="article:published_time" content="2018-11-03" />
  <meta property="article:section" content="data-science" />
  <meta property="article:tag" content="gradient descent" />
  <meta property="article:tag" content="partial derivative" />
  <meta property="article:tag" content="cost function" />
  <meta property="article:tag" content="weights" />
  <meta property="article:tag" content="biases" />
  <meta property="article:tag" content="neural network" />
  <meta property="article:tag" content="deep learning" />
  <meta property="article:tag" content="data science" />


  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="referrer" content="origin" />
  <meta name="generator" content="Pelican" />
  <link href="./" rel="canonical" />

  <!-- Feed -->

  <link href="./theme/css/style.css" type="text/css" rel="stylesheet" />

  <!-- Code highlight color scheme -->
      <link href="./theme/css/code_blocks/github.css" rel="stylesheet">


  <!-- Custom fonts -->
  <link href='https://fonts.googleapis.com/css?family=Montserrat:400,300' rel='stylesheet' type='text/css' />
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css" />

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->


  <link href="./gradient-descent.html" rel="canonical" />

    <meta name="description" content="Gradient descent updates the parameters of a model so that the output of the cost function is reduced.">

    <meta name="author" content="chris dinant">

    <meta name="tags" content="gradient descent">
    <meta name="tags" content="partial derivative">
    <meta name="tags" content="cost function">
    <meta name="tags" content="weights">
    <meta name="tags" content="biases">
    <meta name="tags" content="neural network">
    <meta name="tags" content="deep learning">
    <meta name="tags" content="data science">




<!-- Open Graph -->
<meta property="og:site_name" content="Transferred Learnings"/>
<meta property="og:title" content="What is Gradient Descent?"/>
<meta property="og:description" content="Gradient descent updates the parameters of a model so that the output of the cost function is reduced."/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="./gradient-descent.html"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2018-11-03 00:00:00+01:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="./author/chris-dinant.html">
<meta property="article:section" content="data-science"/>
<meta property="article:tag" content="gradient descent"/>
<meta property="article:tag" content="partial derivative"/>
<meta property="article:tag" content="cost function"/>
<meta property="article:tag" content="weights"/>
<meta property="article:tag" content="biases"/>
<meta property="article:tag" content="neural network"/>
<meta property="article:tag" content="deep learning"/>
<meta property="article:tag" content="data science"/>
<meta property="og:image" content=".//images/descending.jpg">

<!-- Twitter Card -->

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "name": "What is Gradient Descent?",
  "headline": "What is Gradient Descent?",
  "datePublished": "2018-11-03 00:00:00+01:00",
  "dateModified": "",
  "author": {
    "@type": "Person",
    "name": "chris dinant",
    "url": "./author/chris-dinant.html"
  },
  "image": ".//images/descending.jpg",
  "url": "./gradient-descent.html",
  "description": "Gradient descent updates the parameters of a model so that the output of the cost function is reduced."
}
</script>
</head>
<!-- TODO : Body class -->
<body class="home-template">

<nav id="menu">
  <a class="close-button">Close</a>
  <div class="nav-wrapper">
    <p class="nav-label">Menu</p>
    <ul>

              <li role="presentation"><a href="./pages/about-me.html">Whose blog is this?</a></li>
              <li role="presentation"><a href="./pages/purpose.html">The purpose of this blog</a></li>
      
                  <li class="nav-data-science active" role="presentation"><a href="./category/data-science.html">data-science</a></li>
                  <li role="presentation"><a href="./category/not-data-science.html">not-data-science</a></li>

    </ul>
  </div>
</nav>
    <!-- Progressbar -->
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header id="post-header" class="has-cover">
      <div class="inner">
        <nav id="navigation">
            <span class="blog-logo">
              <a href="./"><img src="/extra/delta9.png" alt="Blog Logo" /></a>
            </span>
          <span id="menu-button" class="nav-button">
            <a class="menu-button"><i class="ic ic-menu"></i> Menu</a>
          </span>
        </nav>
        <h1 class="post-title">What is Gradient Descent?</h1>
        <!-- TODO : Proper class for headline -->
          <span class="blog-description">Gradient descent updates the parameters of a model so that the output of the cost function is reduced.</span>
        <span class="post-meta">
                <a href="./author/chris-dinant.html">chris dinant</a>
            | <time datetime="Sat 03 November 2018">Sat 03 November 2018</time>
        </span>
        <!-- TODO : Modified check -->
            <div class="post-cover cover" style="background-image: url('.//images/descending.jpg')">
      </div>
    </header>

  <section id="wrapper">
    <a class="hidden-close"></a>

    <!-- Post content -->
    <main class="content" role="main">
        <article class="post">
        <div class="inner">
            <section class="post-content">
                <p>Fitting a machine learning model means finding optimal values for the parameters of the model. Sometimes this can be done in one step (when a closed form solution is available), but more often than not even if this can be done it is computationally prohibitively expensive. That is why most of the time iterative methods such as gradient descent are used for optimization problems.  </p>
<p>With gradient descent at every iteration the parameters <span class="math">\(w\)</span> and <span class="math">\(b\)</span> (for <em>weights</em> and <em>biases</em>) are adjusted a certain amount in the direction that results in the reduction of the cost function <span class="math">\(J\)</span>. So how do we determine the direction and amount to adjust? We use the gradient of the cost function with regards to the parameters.  </p>
<p>Here's how calculate this gradient:  </p>
<p>The cost function takes the output of the model <span class="math">\(\hat{y}\)</span> and compares it with the actual value of the training sample, <span class="math">\(y\)</span>, to determine how <em>wrong</em> the model is at this stage of training. Often you see functions like <em>mean squared error</em> (MSE) or <em>cross entropy</em> used for this. The <span class="math">\(\hat{y}\)</span> value in turn is determined by the activation  at the output layer (if we are talking about a neural network), which in the case of a binary classification problem would probably be a sigmoid function. And the input to this sigmoid function would be a linear function in the style of </p>
<div class="math">$$z=w_1x_1 + w_2x_2 + ... + w_Nx_N + b$$</div>
<p> for a fully-connected layer with <span class="math">\(N\)</span> neurons. Now, to take the derivative of cost function <span class="math">\(J\)</span> with regards to the parameters you take the (partial) derivatives of <span class="math">\(J\)</span> at each of the above steps and multiply them with each other. This is called the chain rule. It goes as follows:  </p>
<ol>
<li>If we are using cross entropy, the derivative of <span class="math">\(J\)</span> with regards to <span class="math">\(\hat{y}\)</span> is:  <div class="math">$$\frac{\partial{J}}{\partial{\hat{y}}}=-\frac{y}{\hat{y}}+\frac{1-y}{1-\hat{y}}$$</div>
</li>
<li>One step down the derivative of <span class="math">\(\hat{y}\)</span> with regards to the output of the linear function (let's call it <span class="math">\(z\)</span>) is:
<div class="math">$$\frac{\partial{\hat{y}}}{\partial{z}}=\hat{y}(1-\hat{y})$$</div>
</li>
<li>And the partial derivative for the parameters is:
<div class="math">$$\frac{\partial{z}}{\partial{w_n}}=x_n$$</div> for all the weights and simply <span class="math">\(1\)</span> for bias <span class="math">\(b\)</span>.  </li>
</ol>
<p>A funny thing about using a sigmoid activation function in the output layer and cross entropy as the loss function is that with the chain rule this end of the network simplifies to:
</p>
<div class="math">$$\frac{\partial{J}}{\partial{w_n}}=\frac{\partial{J}}{\partial{\hat{y}}}\frac{\partial{\hat{y}}}{\partial{z}}\frac{\partial{z}}{\partial{w_n}}=x_n(\hat{y}-y)$$</div>
<p>
for the weights and <span class="math">\(\hat{y}-y\)</span> for the bias. Isn't that nice and simple?</p>
<p>Now we can adjust the parameters as follows:
</p>
<div class="math">$$\Delta{w_n}=-\alpha{\frac{\partial{J}}{\partial{w_n}}}$$</div>
<p>
We use a small scalar <span class="math">\(\alpha\)</span> here, always <span class="math">\(&lt;1\)</span> and often <span class="math">\(&lt;0.01\)</span>, in order not to update the parameters in too large steps because this usually leads to increasing <span class="math">\(J\)</span> instead of decreasing it. This <span class="math">\(\alpha\)</span> is called the <em>learning rate</em>.  </p>
<hr/>
<p>With multi-layer networks the <span class="math">\(x_n\)</span> in the above equations is equivalent to the output of the previous activation layer. So to continue back propagation into the upstream layer you take the derivative of <span class="math">\(z\)</span> with regards to this activation (let's call it <span class="math">\(a^{[l-1]}\)</span> instead of <span class="math">\(x\)</span> now to make clear that we are not dealing with the features of the input sample but the activation of upstream layer <span class="math">\(l-1\)</span>):
</p>
<div class="math">$$\frac{\partial{z}}{\partial{a_n^{[l-1]}}}=w_n$$</div>
<p>Next we need the derivative of this activation function <span class="math">\(a^{[l-1]}\)</span> with regards to <span class="math">\(z^{[l-1]}\)</span>. Obviously the formula for this derivation depends on which activation function you choose. Here are some common ones:  </p>
<table>
<thead>
<tr>
<th>function</th>
<th align="center"><div style="text-align: center">equation</div></th>
<th align="center"><div style="text-align: center">derivative</div></th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear</td>
<td align="center">
<div class="math">$$f(x)=x$$</div>
</td>
<td align="center">
<div class="math">$$f'(x)=1$$</div>
</td>
</tr>
<tr>
<td>Sigmoid</td>
<td align="center">
<div class="math">$$f(x)=\frac{1}{1+e^{-x}}$$</div>
</td>
<td align="center">
<div class="math">$$f'(x)=f(x)(1-f(x))$$</div>
</td>
</tr>
<tr>
<td>ReLU</td>
<td align="center">
<div class="math">$$f(x)=\begin{cases}  0 &amp; \text{for }x&lt;0\\  x &amp; \text{for }x\ge0\end{cases}$$</div>
</td>
<td align="center">
<div class="math">$$f(x)=\begin{cases}  0 &amp; \text{for }x&lt;0\\  1 &amp; \text{for }x\ge0\end{cases}$$</div>
</td>
</tr>
<tr>
<td>Tanh</td>
<td align="center">
<div class="math">$$f(x)=\frac{2}{1+e^{-2x}}-1$$</div>
</td>
<td align="center">
<div class="math">$$f'(x)=1-f(x)^2$$</div>
</td>
</tr>
</tbody>
</table>
<p>Say we use Tanh:
</p>
<div class="math">$$\frac{\partial{a_n^{[l-1]}}}{\partial{z^{[l-1]}}}=1-a_n^{[l-1]2}$$</div>
<p>
And finally the derivative with regards to the parameters of layer <span class="math">\(l-1\)</span>:
</p>
<div class="math">$$\frac{\partial{z^{[l-1]}}}{\partial{w_n^{[l-1]}}}=x_n^{[l-1]}$$</div>
<p>Again we apply the chain rule to update of the weights in layer <span class="math">\(l-1\)</span>:
</p>
<div class="math">$$\Delta{w_n^{[l-1]}}=-\alpha{\frac{\partial{J}}{\partial{w_n^{[l-1]}}}}=-\alpha{x_n^{[l-1]}(1-a_n^{[l-1]2})w_n(\hat{y}-y)}$$</div>
<p>
where <span class="math">\(a_n^{[l-1]}\)</span> is the output of tanh<span class="math">\((x^{[l-1]})\)</span> and <span class="math">\(\hat{y}\)</span> the output of sigmoid<span class="math">\((x)\)</span>.  </p>
<hr/>
<p>Now we have seen how to do back propagation by gradient descent for a two-layer neural network. I have not talked about how to deal with batches of data and how to vectorize these calculations for faster computation. We will have to wait for a later post to talk about those topics.   </p>
<p>Î”9</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
            </section>

            <section class="post-info">
                <div class="post-share">
                    <a class="twitter" href="https://twitter.com/share?text=What is Gradient Descent?&amp;url=./gradient-descent.html" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <i class="ic ic-twitter"></i><span class="hidden">Twitter</span>
                    </a>
                    <a class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=./gradient-descent.html" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <i class="ic ic-facebook"></i><span class="hidden">Facebook</span>
                    </a>
                    <a class="googleplus" href="https://plus.google.com/share?url=./gradient-descent.html" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <i class="ic ic-googleplus"></i><span class="hidden">Google+</span>
                    </a>
                    <div class="clear"></div>
                </div>

                <aside class="post-tags">
<a href="./tag/gradient-descent.html">gradient descent</a><a href="./tag/partial-derivative.html">partial derivative</a><a href="./tag/cost-function.html">cost function</a><a href="./tag/weights.html">weights</a><a href="./tag/biases.html">biases</a><a href="./tag/neural-network.html">neural network</a><a href="./tag/deep-learning.html">deep learning</a><a href="./tag/data-science.html">data science</a>                </aside>

                <div class="clear"></div>

                <aside class="post-author">


                        <figure class="post-author-avatar">
                            <img src="./extra/delta9.png" alt="chris dinant" />
                        </figure>
                    <div class="post-author-bio">
                        <h4 class="post-author-name"><a href="./author/chris-dinant.html">chris dinant</a></h4>
                            <p class="post-author-about">Only if you can explain something in simple terms do you really understand it. That's what I'm trying to do with this blog.</p>
                            <span class="post-author-location"><i class="ic ic-location"></i> Copenhagen</span>
                        <!-- Social linkes in alphabet order. -->
                            <span class="post-author-github"><a target="_blank" href="https://github.com/chrisdinant"><i class="ic ic-link"></i> GitHub</a></span>
                            <span class="post-author-linkedin"><a target="_blank" href="https://www.linkedin.com/in/chris-dinant"><i class="ic ic-link"></i> LinkedIn</a></span>
                            <span class="post-author-twitter"><a target="_blank" href="https://twitter.com/chrisdinant"><i class="ic ic-twitter"></i> Twitter</a></span>
                    </div>
                    <div class="clear"></div>
                </aside>

                </section>

                <script type="text/javascript">
                    var disqus = 'chrisdinant';
                    var disqus_shortname = 'chrisdinant';
                    var disqus_identifier = '/gradient-descent.html';
                    var disqus_url = './gradient-descent.html';
                </script>
                <noscript>Please enable JavaScript to view the comments.</noscript>
                <section class="post-comments">
                        <a id="show-disqus" class="post-comments-activate" data-disqus-identifier="/gradient-descent.html" >Show Comments</a>
                    <div id="disqus_thread"></div>
                </section>

                <aside class="post-nav">
                    <a class="post-nav-prev" href="./custom-pixels.html">
                        <section class="post-nav-teaser">
                            <i class="ic ic-arrow-right"></i>
                                <h2 class="post-nav-title">Music Video</h2>
                            <p class="post-nav-excerpt">I made a script that replaces pixels in movies with 20x20 size jpgs.</p>
                        </section>
                    </a>
                    <div class="clear"></div>
                </aside>

            </div>
        </article>
    </main>
      <!-- TODO : Body class -->
    <div id="body-class" style="display: none;" class=""></div>

    <footer id="footer">
      <div class="inner">
        <section class="credits">


          <span class="credits-theme">Theme <a href="https://github.com/arulrajnet/attila" rel="nofollow">Attila</a></span>
          <span class="credits-software">Published with <a href="https://github.com/getpelican/pelican" rel="nofollow">Pelican</a></span>
        </section>
      </div>
    </footer>
  </section>

  <script type="text/javascript" src="./theme/js/script.js"></script>

<script type="text/javascript">
    var disqus_shortname = 'chrisdinant';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
</body>
</html>